{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMqXyQ4byjxZD08D7vertl6"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJ5vpvOUiAue",
        "outputId": "77186afa-d04c-46ac-f30c-c09ab222543c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x78b0c1c0c8b0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "sImX6lzfj50h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The Doc Object for Processed Text"
      ],
      "metadata": {
        "id": "Yf8yISWbjdk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "introduction_doc = nlp(\n",
        "...     \"This tutorial is about Natural Language Processing in spaCy.\"\n",
        "... )\n",
        "type(introduction_doc)\n",
        "[token.text for token in introduction_doc]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UF6F4WSliNnz",
        "outputId": "40fc18c6-1993-4389-a2d5-5c3cce9ea648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'tutorial',\n",
              " 'is',\n",
              " 'about',\n",
              " 'Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " 'in',\n",
              " 'spaCy',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "file_name = \"/content/introduction.txt\"\n",
        "introduction_doc = nlp(pathlib.Path(file_name).read_text(encoding=\"utf-8\"))\n",
        "print ([token.text for token in introduction_doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKYoqD99is0H",
        "outputId": "8ebd2513-145c-4bbc-d6ea-4e232e019152"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\"', 'This', 'tutorial', 'is', 'about', 'Natural', 'Language', 'Processing', 'in', 'spaCy', '.', '\"']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ylocSCVNj4pH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sentence Detection"
      ],
      "metadata": {
        "id": "RidNRn9EjX-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "about_text = (\n",
        "    \"Gus Proto is a Python developer currently\"\n",
        "    \" working for a London-based Fintech\"\n",
        "    \" company. He is interested in learning\"\n",
        "    \" Natural Language Processing.\"\n",
        ")\n",
        "about_doc = nlp(about_text)\n",
        "sentences = list(about_doc.sents)\n",
        "len(sentences)\n",
        "\n",
        "for sentence in sentences:\n",
        "    print(f\"{sentence[:5]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfFWWWJMjLpI",
        "outputId": "f04a31dd-2bbc-49e0-919e-adc2754e8ebd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gus Proto is a Python...\n",
            "He is interested in learning...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ellipsis_text = (\n",
        "    \"Gus, can you, ... never mind, I forgot\"\n",
        "    \" what I was saying. So, do you think\"\n",
        "    \" we should ...\"\n",
        ")\n",
        "\n",
        "from spacy.language import Language\n",
        "@Language.component(\"set_custom_boundaries\")\n",
        "def set_custom_boundaries(doc):\n",
        "    \"\"\"Add support to use `...` as a delimiter for sentence detection\"\"\"\n",
        "    for token in doc[:-1]:\n",
        "        if token.text == \"...\":\n",
        "            doc[token.i + 1].is_sent_start = True\n",
        "    return doc\n",
        "\n",
        "\n",
        "custom_nlp = spacy.load(\"en_core_web_sm\")\n",
        "custom_nlp.add_pipe(\"set_custom_boundaries\", before=\"parser\")\n",
        "custom_ellipsis_doc = custom_nlp(ellipsis_text)\n",
        "custom_ellipsis_sentences = list(custom_ellipsis_doc.sents)\n",
        "for sentence in custom_ellipsis_sentences:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4V3fefeejTKQ",
        "outputId": "d5b16c46-936e-47fd-b571-323c52e58e93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gus, can you, ...\n",
            "never mind, I forgot what I was saying.\n",
            "So, do you think we should ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We used the @Language.component(\"set_custom_boundaries\") decorator to define a new function that takes a Doc object as an argument. The job of this function is to identify tokens in Doc that are the beginning of sentences and mark their .is_sent_start attribute to True. Once done, the function must return the Doc object again"
      ],
      "metadata": {
        "id": "sCflvNsCjyCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9cu-pL5cj7ym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tokens in spaCy"
      ],
      "metadata": {
        "id": "swqnwUIlkSt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the Doc container involves tokenizing the text. The process of tokenization breaks a text down into its basic units—or tokens—which are represented in spaCy as Token objects."
      ],
      "metadata": {
        "id": "fdM8jEVZkXjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "about_text = (\n",
        "    \"Gus Proto is a Python developer currently\"\n",
        "    \" working for a London-based Fintech\"\n",
        "    \" company. He is interested in learning\"\n",
        "    \" Natural Language Processing.\"\n",
        ")\n",
        "about_doc = nlp(about_text)\n",
        "\n",
        "for token in about_doc:\n",
        "    print (token, token.idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEgHFegaj9Ft",
        "outputId": "0e8ed91b-9cb3-493a-b194-22014ab5a50f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gus 0\n",
            "Proto 4\n",
            "is 10\n",
            "a 13\n",
            "Python 15\n",
            "developer 22\n",
            "currently 32\n",
            "working 42\n",
            "for 50\n",
            "a 54\n",
            "London 56\n",
            "- 62\n",
            "based 63\n",
            "Fintech 69\n",
            "company 77\n",
            ". 84\n",
            "He 86\n",
            "is 89\n",
            "interested 92\n",
            "in 103\n",
            "learning 106\n",
            "Natural 115\n",
            "Language 123\n",
            "Processing 132\n",
            ". 142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{\"Text with Whitespace\":22}'\n",
        "      f'{\"Is Alphanumeric?\":15}'\n",
        "      f'{\"Is Punctuation?\":18}'\n",
        "      f'{\"Is Stop Word?\"}'\n",
        ")\n",
        "\n",
        "for token in about_doc:\n",
        "    print(\n",
        "        f\"{str(token.text_with_ws):22}\"\n",
        "        f\"{str(token.is_alpha):15}\"\n",
        "        f\"{str(token.is_punct):18}\"\n",
        "        f\"{str(token.is_stop)}\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lrjzWLEkea8",
        "outputId": "a7d062f2-7cd1-4093-ffc6-f094bbe7f68a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text with Whitespace  Is Alphanumeric?Is Punctuation?   Is Stop Word?\n",
            "Gus                   True           False             False\n",
            "Proto                 True           False             False\n",
            "is                    True           False             True\n",
            "a                     True           False             True\n",
            "Python                True           False             False\n",
            "developer             True           False             False\n",
            "currently             True           False             False\n",
            "working               True           False             False\n",
            "for                   True           False             True\n",
            "a                     True           False             True\n",
            "London                True           False             False\n",
            "-                     False          True              False\n",
            "based                 True           False             False\n",
            "Fintech               True           False             False\n",
            "company               True           False             False\n",
            ".                     False          True              False\n",
            "He                    True           False             True\n",
            "is                    True           False             True\n",
            "interested            True           False             False\n",
            "in                    True           False             True\n",
            "learning              True           False             False\n",
            "Natural               True           False             False\n",
            "Language              True           False             False\n",
            "Processing            True           False             False\n",
            ".                     False          True              False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* text_with_ws prints the token text along with any trailing space, if present\n",
        "* is_alpha indicates whether the token consists of alphabetic characters or not\n",
        "* is_punct indicates whether the token is a punctuation symbol or not\n",
        "* is_stop indicates whether the token is a stop word or not"
      ],
      "metadata": {
        "id": "s4KkWd-VklMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_about_text = (\n",
        "    \"Gus Proto is a Python developer currently\"\n",
        "    \" working for a London@based Fintech\"\n",
        "    \" company. He is interested in learning\"\n",
        "    \" Natural Language Processing.\"\n",
        ")\n",
        "\n",
        "print([token.text for token in nlp(custom_about_text)[8:15]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRcNtGslkvCX",
        "outputId": "c97f5e60-0aa2-4fa1-e4aa-6fd1da7a5a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['for', 'a', 'London@based', 'Fintech', 'company', '.', 'He']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* As with many aspects of spaCy, we can also customize the tokenization process to detect tokens on custom characters. This is often used for hyphenated words such as London-based.\n",
        "\n",
        "* To customize tokenization, we need to update the tokenizer property on the callable Language object with a new Tokenizer object.\n",
        "\n",
        "* To see what's involved, imagine we had some text that used the @ symbol instead of the usual hyphen (-) as an infix to link words together. So, instead of London-based, we had London@based\n",
        "\n",
        "* In this example, the default parsing read the London@based text as a single token, but if we had used a hyphen instead of the @ symbol, then we'd get three tokens.\n",
        "\n",
        "* To include the @ symbol as a custom infix, we need to build our own Tokenizer object"
      ],
      "metadata": {
        "id": "z9G5wuZYmbtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from spacy.tokenizer import Tokenizer\n",
        "\n",
        "custom_nlp = spacy.load(\"en_core_web_sm\")\n",
        "prefix_re = spacy.util.compile_prefix_regex(\n",
        "    custom_nlp.Defaults.prefixes\n",
        ")\n",
        "suffix_re = spacy.util.compile_suffix_regex(\n",
        "    custom_nlp.Defaults.suffixes\n",
        ")\n",
        "\n",
        "custom_infixes = [r\"@\"]\n",
        "\n",
        "infix_re = spacy.util.compile_infix_regex(\n",
        "    list(custom_nlp.Defaults.infixes) + custom_infixes\n",
        ")\n",
        "\n",
        "custom_nlp.tokenizer = Tokenizer(\n",
        "    nlp.vocab,\n",
        "    prefix_search=prefix_re.search,\n",
        "    suffix_search=suffix_re.search,\n",
        "    infix_finditer=infix_re.finditer,\n",
        "    token_match=None,\n",
        ")\n",
        "\n",
        "custom_tokenizer_about_doc = custom_nlp(custom_about_text)\n",
        "\n",
        "print([token.text for token in custom_tokenizer_about_doc[8:15]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICjqNvs2mh-h",
        "outputId": "eb6fa3fc-6c16-4627-e63a-6c649d4c2055"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['for', 'a', 'London', '@', 'based', 'Fintech', 'company']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Stop Words"
      ],
      "metadata": {
        "id": "EueV0XGenAvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop words are typically defined as the most common words in a language. In the English language, some examples of stop words are the, are, but, and they. Most sentences need to contain stop words in order to be full sentences that make grammatical sense.\n",
        "\n",
        "With NLP, stop words are generally removed because they aren’t significant, and they heavily distort any word frequency analysis. spaCy stores a list of stop words for the English language"
      ],
      "metadata": {
        "id": "lch4S9vOnIeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "len(spacy_stopwords)\n",
        "\n",
        "for stop_word in list(spacy_stopwords)[:10]:\n",
        "    print(stop_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUC5FgSOm850",
        "outputId": "df0e52da-c0bb-4cbc-ba5a-1a0a3086f375"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "across\n",
            "'ve\n",
            "hers\n",
            "beforehand\n",
            "re\n",
            "six\n",
            "namely\n",
            "is\n",
            "’ve\n",
            "we\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_about_text = (\n",
        "    \"Gus Proto is a Python developer currently\"\n",
        "    \" working for a London-based Fintech\"\n",
        "    \" company. He is interested in learning\"\n",
        "    \" Natural Language Processing.\"\n",
        ")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "about_doc = nlp(custom_about_text)\n",
        "print([token for token in about_doc if not token.is_stop])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7IMlgwBnMy-",
        "outputId": "0759e525-4a57-47cf-b2c4-6e6c26dd610e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gus, Proto, Python, developer, currently, working, London, -, based, Fintech, company, ., interested, learning, Natural, Language, Processing, .]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Lemmatization"
      ],
      "metadata": {
        "id": "bEN54-6InQL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization is the process of reducing inflected forms of a word while still ensuring that the reduced form belongs to the language. This reduced form, or root word, is called a lemma."
      ],
      "metadata": {
        "id": "vTK_GognnS_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "conference_help_text = (\n",
        "    \"Gus is helping organize a developer\"\n",
        "    \" conference on Applications of Natural Language\"\n",
        "    \" Processing. He keeps organizing local Python meetups\"\n",
        "    \" and several internal talks at his workplace.\"\n",
        ")\n",
        "conference_help_doc = nlp(conference_help_text)\n",
        "for token in conference_help_doc:\n",
        "    if str(token) != str(token.lemma_):\n",
        "        print(f\"{str(token):>20} : {str(token.lemma_)}\")"
      ],
      "metadata": {
        "id": "2B7xwB85nRYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Word Embeddings"
      ],
      "metadata": {
        "id": "B8mHVWLnoThR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word embedding techniques - Word2Vec, GloVe, and FastText using Gensim library in Python."
      ],
      "metadata": {
        "id": "3rQfimw-oboJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "sentences = [['I', 'love', 'machine', 'learning'],\n",
        "             ['Word', 'embeddings', 'are', 'powerful'],\n",
        "             ['Natural', 'language', 'processing', 'is', 'interesting']]\n",
        "\n",
        "model_w2v = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "word_vector = model_w2v.wv['learning']\n",
        "print(word_vector)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAdzC-aZoXCe",
        "outputId": "7c860bbd-067d-4939-fb33-ab6bbe1c27e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.00515624 -0.00666834 -0.00777684  0.00831073 -0.00198234 -0.00685496\n",
            " -0.00415439  0.00514413 -0.00286914 -0.00374966  0.00162143 -0.00277629\n",
            " -0.00158436  0.00107449 -0.00297794  0.00851928  0.00391094 -0.00995886\n",
            "  0.0062596  -0.00675425  0.00076943  0.00440423 -0.00510337 -0.00211067\n",
            "  0.00809548 -0.00424379 -0.00763626  0.00925791 -0.0021555  -0.00471943\n",
            "  0.0085708   0.00428334  0.00432484  0.00928451 -0.00845308  0.00525532\n",
            "  0.00203935  0.00418828  0.0016979   0.00446413  0.00448629  0.00610452\n",
            " -0.0032021  -0.00457573 -0.00042652  0.00253373 -0.00326317  0.00605772\n",
            "  0.00415413  0.00776459  0.00256927  0.00811668 -0.00138721  0.00807793\n",
            "  0.00371702 -0.00804732 -0.00393361 -0.00247188  0.00489304 -0.00087216\n",
            " -0.00283091  0.00783371  0.0093229  -0.00161493 -0.00515925 -0.00470176\n",
            " -0.00484605 -0.00960283  0.00137202 -0.00422492  0.00252671  0.00561448\n",
            " -0.00406591 -0.00959658  0.0015467  -0.00670012  0.00249517 -0.00378063\n",
            "  0.00707842  0.00064022  0.00356094 -0.00273913 -0.00171055  0.00765279\n",
            "  0.00140768 -0.00585045 -0.0078345   0.00123269  0.00645463  0.00555635\n",
            " -0.00897705  0.00859216  0.00404698  0.00746961  0.00974633 -0.00728958\n",
            " -0.00903996  0.005836    0.00939121  0.00350693]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "sentences = [['I', 'love', 'machine', 'learning'],\n",
        "             ['Word', 'embeddings', 'are', 'powerful'],\n",
        "             ['Natural', 'language', 'processing', 'is', 'interesting']]\n",
        "\n",
        "model_fasttext = FastText(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "word_vector = model_fasttext.wv['learning']\n",
        "print(word_vector)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IiTmUYemohbV",
        "outputId": "6002c31f-ceb8-4ffa-f2c2-80cf69db580b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 9.1823016e-04 -3.0900480e-04 -1.4241622e-03  3.6915974e-04\n",
            " -1.6989922e-03 -4.7368856e-04 -9.6330949e-04 -8.1712339e-04\n",
            "  1.4180044e-03  3.7115382e-04 -1.1983200e-04  2.9081755e-04\n",
            " -9.9326216e-04  1.0794357e-03  5.1063707e-04 -1.1530268e-03\n",
            " -1.5505445e-03 -1.9562184e-03  5.7489559e-04 -2.0934530e-03\n",
            " -1.8944317e-03 -2.0447427e-03  1.0821981e-03 -2.2537062e-04\n",
            " -5.0764071e-04 -1.0467614e-03 -6.9561160e-05 -7.7934202e-04\n",
            " -3.2007642e-04 -1.4727070e-03 -1.4016990e-03  1.8091477e-03\n",
            "  7.5201882e-04  4.1703676e-04 -1.6488129e-04  1.2592709e-03\n",
            " -4.0854918e-04  5.9077330e-04 -1.3326405e-03 -8.8910357e-04\n",
            " -1.0974610e-03 -1.0485642e-03 -7.7783951e-04  6.8859063e-04\n",
            " -1.1026027e-03  3.0985163e-04 -8.7897031e-05  4.3270687e-04\n",
            "  2.0680524e-04  7.2805583e-04  1.5761263e-03 -4.8463579e-04\n",
            " -3.9394345e-04  1.9359115e-03  9.9428324e-04  7.9648633e-04\n",
            " -2.5278129e-04  2.9308398e-04 -6.4702188e-05 -9.9841051e-04\n",
            "  3.2134666e-04 -6.8074651e-04 -1.5585172e-03  1.4566220e-03\n",
            " -1.5276216e-03 -1.0355375e-03  1.3160633e-03  3.2075797e-04\n",
            " -3.2630994e-04 -5.8626203e-04  1.2398914e-03  3.6867801e-04\n",
            " -1.0769401e-03 -1.4016590e-03 -3.2425561e-04 -7.0824503e-04\n",
            "  1.6528033e-03  1.4326158e-03 -1.1425486e-03  1.1718721e-03\n",
            "  2.1578651e-03  1.4469103e-03 -1.5460409e-03  2.1388366e-04\n",
            " -9.5607113e-04  6.4851344e-04  9.8851381e-04 -3.9630383e-04\n",
            " -2.8651973e-04  7.5931143e-04  9.5744169e-04  4.8965204e-04\n",
            "  4.3499912e-04  5.8338168e-04  1.3906470e-03  1.0010041e-03\n",
            " -5.7730591e-04  1.9127502e-03 -5.8024749e-04 -9.3893014e-04]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "FastText with spacy:"
      ],
      "metadata": {
        "id": "-XqRkfjZpizV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "import spacy\n",
        "\n",
        "# Sample sentences\n",
        "sentences = ['I love machine learning.',\n",
        "             'Word embeddings are powerful.',\n",
        "             'Natural language processing is interesting.']\n",
        "\n",
        "# Tokenize and preprocess the sentences using spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "preprocessed_sentences = [[token.text.lower() for token in nlp(sentence) if token.is_alpha] for sentence in sentences]\n",
        "\n",
        "# Train FastText model\n",
        "model_fasttext = FastText(sentences=preprocessed_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Get word vector for 'learning'\n",
        "word_vector = model_fasttext.wv['learning']\n",
        "print(word_vector)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lh0pRufupfNl",
        "outputId": "d36d0782-8ca5-4b2f-b877-d5fa1a436690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 9.1823016e-04 -3.0900480e-04 -1.4241622e-03  3.6915974e-04\n",
            " -1.6989922e-03 -4.7368856e-04 -9.6330949e-04 -8.1712339e-04\n",
            "  1.4180044e-03  3.7115382e-04 -1.1983200e-04  2.9081755e-04\n",
            " -9.9326216e-04  1.0794357e-03  5.1063707e-04 -1.1530268e-03\n",
            " -1.5505445e-03 -1.9562184e-03  5.7489559e-04 -2.0934530e-03\n",
            " -1.8944317e-03 -2.0447427e-03  1.0821981e-03 -2.2537062e-04\n",
            " -5.0764071e-04 -1.0467614e-03 -6.9561160e-05 -7.7934202e-04\n",
            " -3.2007642e-04 -1.4727070e-03 -1.4016990e-03  1.8091477e-03\n",
            "  7.5201882e-04  4.1703676e-04 -1.6488129e-04  1.2592709e-03\n",
            " -4.0854918e-04  5.9077330e-04 -1.3326405e-03 -8.8910357e-04\n",
            " -1.0974610e-03 -1.0485642e-03 -7.7783951e-04  6.8859063e-04\n",
            " -1.1026027e-03  3.0985163e-04 -8.7897031e-05  4.3270687e-04\n",
            "  2.0680524e-04  7.2805583e-04  1.5761263e-03 -4.8463579e-04\n",
            " -3.9394345e-04  1.9359115e-03  9.9428324e-04  7.9648633e-04\n",
            " -2.5278129e-04  2.9308398e-04 -6.4702188e-05 -9.9841051e-04\n",
            "  3.2134666e-04 -6.8074651e-04 -1.5585172e-03  1.4566220e-03\n",
            " -1.5276216e-03 -1.0355375e-03  1.3160633e-03  3.2075797e-04\n",
            " -3.2630994e-04 -5.8626203e-04  1.2398914e-03  3.6867801e-04\n",
            " -1.0769401e-03 -1.4016590e-03 -3.2425561e-04 -7.0824503e-04\n",
            "  1.6528033e-03  1.4326158e-03 -1.1425486e-03  1.1718721e-03\n",
            "  2.1578651e-03  1.4469103e-03 -1.5460409e-03  2.1388366e-04\n",
            " -9.5607113e-04  6.4851344e-04  9.8851381e-04 -3.9630383e-04\n",
            " -2.8651973e-04  7.5931143e-04  9.5744169e-04  4.8965204e-04\n",
            "  4.3499912e-04  5.8338168e-04  1.3906470e-03  1.0010041e-03\n",
            " -5.7730591e-04  1.9127502e-03 -5.8024749e-04 -9.3893014e-04]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Word Frequency"
      ],
      "metadata": {
        "id": "RTxKY4GzqJ7i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now convert a given text into tokens and perform statistical analysis on it. This analysis can give us various insights, such as common words or unique words in the text:"
      ],
      "metadata": {
        "id": "xYDpEZVoqSvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "complete_text = (\n",
        "    \"Gus Proto is a Python developer currently\"\n",
        "    \" working for a London-based Fintech company. He is\"\n",
        "    \" interested in learning Natural Language Processing.\"\n",
        "    \" There is a developer conference happening on 21 July\"\n",
        "    ' 2019 in London. It is titled \"Applications of Natural'\n",
        "    ' Language Processing\". There is a helpline number'\n",
        "    \" available at +44-1234567891. Gus is helping organize it.\"\n",
        "    \" He keeps organizing local Python meetups and several\"\n",
        "    \" internal talks at his workplace. Gus is also presenting\"\n",
        "    ' a talk. The talk will introduce the reader about \"Use'\n",
        "    ' cases of Natural Language Processing in Fintech\".'\n",
        "    \" Apart from his work, he is very passionate about music.\"\n",
        "    \" Gus is learning to play the Piano. He has enrolled\"\n",
        "    \" himself in the weekend batch of Great Piano Academy.\"\n",
        "    \" Great Piano Academy is situated in Mayfair or the City\"\n",
        "    \" of London and has world-class piano instructors.\"\n",
        ")\n",
        "complete_doc = nlp(complete_text)\n",
        "\n",
        "words = [\n",
        "    token.text\n",
        "    for token in complete_doc\n",
        "    if not token.is_stop and not token.is_punct\n",
        "]\n",
        "\n",
        "print(Counter(words).most_common(5))\n"
      ],
      "metadata": {
        "id": "_oGPziwYpgJY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e913814-25f3-4ec2-c11e-26dd2179bcf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Gus', 4), ('London', 3), ('Natural', 3), ('Language', 3), ('Processing', 3)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By looking just at the common words, we can probably assume that the text is about Gus, London, and Natural Language Processing. That’s a significant finding! If we can just look at the most common words, that may save us a lot of reading, because we can immediately tell if the text is about something that interests us or not."
      ],
      "metadata": {
        "id": "3zCXa2ghqi6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Counter(\n",
        "    [token.text for token in complete_doc if not token.is_punct]\n",
        ").most_common(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3pZJnHtqZbE",
        "outputId": "cbef1b96-174d-4662-dae5-2bfcfc6e719e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('is', 10), ('a', 5), ('in', 5), ('Gus', 4), ('of', 4)]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Four out of five of the most common words are stop words that don’t really tell us much about the summarized text. This is why stop words are often considered noise for many applications."
      ],
      "metadata": {
        "id": "ZFYmXzQ7qxiE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Part of Speech Tagging"
      ],
      "metadata": {
        "id": "Iw6TET0bq1R5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part of speech or POS is a grammatical role that explains how a particular word is used in a sentence. There are typically eight parts of speech:\n",
        "\n",
        "* Noun\n",
        "* Pronoun\n",
        "* Adjective\n",
        "* Verb\n",
        "* Adverb\n",
        "* Preposition\n",
        "* Conjunction\n",
        "* Interjection"
      ],
      "metadata": {
        "id": "vYlq4YbBq9KC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part-of-speech tagging is the process of assigning a POS tag to each token depending on its usage in the sentence. POS tags are useful for assigning a syntactic category like noun or verb to each word"
      ],
      "metadata": {
        "id": "qwdC_d0zrIDK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In spaCy, POS tags are available as an attribute on the Token object:"
      ],
      "metadata": {
        "id": "prB1bjhDrLSW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "about_text = (\n",
        "    \"Gus Proto is a Python developer currently\"\n",
        "    \" working for a London-based Fintech\"\n",
        "    \" company. He is interested in learning\"\n",
        "    \" Natural Language Processing.\"\n",
        ")\n",
        "about_doc = nlp(about_text)\n",
        "for token in about_doc:\n",
        "    print(\n",
        "        f\"\"\"\n",
        "TOKEN: {str(token)}\n",
        "=====\n",
        "TAG: {str(token.tag_):10} POS: {token.pos_}\n",
        "EXPLANATION: {spacy.explain(token.tag_)}\"\"\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0xIh3DlNqx7R",
        "outputId": "70537324-158c-4c17-f350-eb946064a0e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TOKEN: Gus\n",
            "=====\n",
            "TAG: NNP        POS: PROPN\n",
            "EXPLANATION: noun, proper singular\n",
            "\n",
            "TOKEN: Proto\n",
            "=====\n",
            "TAG: NNP        POS: PROPN\n",
            "EXPLANATION: noun, proper singular\n",
            "\n",
            "TOKEN: is\n",
            "=====\n",
            "TAG: VBZ        POS: AUX\n",
            "EXPLANATION: verb, 3rd person singular present\n",
            "\n",
            "TOKEN: a\n",
            "=====\n",
            "TAG: DT         POS: DET\n",
            "EXPLANATION: determiner\n",
            "\n",
            "TOKEN: Python\n",
            "=====\n",
            "TAG: NNP        POS: PROPN\n",
            "EXPLANATION: noun, proper singular\n",
            "\n",
            "TOKEN: developer\n",
            "=====\n",
            "TAG: NN         POS: NOUN\n",
            "EXPLANATION: noun, singular or mass\n",
            "\n",
            "TOKEN: currently\n",
            "=====\n",
            "TAG: RB         POS: ADV\n",
            "EXPLANATION: adverb\n",
            "\n",
            "TOKEN: working\n",
            "=====\n",
            "TAG: VBG        POS: VERB\n",
            "EXPLANATION: verb, gerund or present participle\n",
            "\n",
            "TOKEN: for\n",
            "=====\n",
            "TAG: IN         POS: ADP\n",
            "EXPLANATION: conjunction, subordinating or preposition\n",
            "\n",
            "TOKEN: a\n",
            "=====\n",
            "TAG: DT         POS: DET\n",
            "EXPLANATION: determiner\n",
            "\n",
            "TOKEN: London\n",
            "=====\n",
            "TAG: NNP        POS: PROPN\n",
            "EXPLANATION: noun, proper singular\n",
            "\n",
            "TOKEN: -\n",
            "=====\n",
            "TAG: HYPH       POS: PUNCT\n",
            "EXPLANATION: punctuation mark, hyphen\n",
            "\n",
            "TOKEN: based\n",
            "=====\n",
            "TAG: VBN        POS: VERB\n",
            "EXPLANATION: verb, past participle\n",
            "\n",
            "TOKEN: Fintech\n",
            "=====\n",
            "TAG: NNP        POS: PROPN\n",
            "EXPLANATION: noun, proper singular\n",
            "\n",
            "TOKEN: company\n",
            "=====\n",
            "TAG: NN         POS: NOUN\n",
            "EXPLANATION: noun, singular or mass\n",
            "\n",
            "TOKEN: .\n",
            "=====\n",
            "TAG: .          POS: PUNCT\n",
            "EXPLANATION: punctuation mark, sentence closer\n",
            "\n",
            "TOKEN: He\n",
            "=====\n",
            "TAG: PRP        POS: PRON\n",
            "EXPLANATION: pronoun, personal\n",
            "\n",
            "TOKEN: is\n",
            "=====\n",
            "TAG: VBZ        POS: AUX\n",
            "EXPLANATION: verb, 3rd person singular present\n",
            "\n",
            "TOKEN: interested\n",
            "=====\n",
            "TAG: JJ         POS: ADJ\n",
            "EXPLANATION: adjective (English), other noun-modifier (Chinese)\n",
            "\n",
            "TOKEN: in\n",
            "=====\n",
            "TAG: IN         POS: ADP\n",
            "EXPLANATION: conjunction, subordinating or preposition\n",
            "\n",
            "TOKEN: learning\n",
            "=====\n",
            "TAG: VBG        POS: VERB\n",
            "EXPLANATION: verb, gerund or present participle\n",
            "\n",
            "TOKEN: Natural\n",
            "=====\n",
            "TAG: NNP        POS: PROPN\n",
            "EXPLANATION: noun, proper singular\n",
            "\n",
            "TOKEN: Language\n",
            "=====\n",
            "TAG: NNP        POS: PROPN\n",
            "EXPLANATION: noun, proper singular\n",
            "\n",
            "TOKEN: Processing\n",
            "=====\n",
            "TAG: NNP        POS: PROPN\n",
            "EXPLANATION: noun, proper singular\n",
            "\n",
            "TOKEN: .\n",
            "=====\n",
            "TAG: .          POS: PUNCT\n",
            "EXPLANATION: punctuation mark, sentence closer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By using POS tags, we can extract a particular category of words:"
      ],
      "metadata": {
        "id": "To3vAZfQrXgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nouns = []\n",
        "adjectives = []\n",
        "for token in about_doc:\n",
        "    if token.pos_ == \"NOUN\":\n",
        "        nouns.append(token)\n",
        "    if token.pos_ == \"ADJ\":\n",
        "        adjectives.append(token)\n"
      ],
      "metadata": {
        "id": "jLDjao8DrYHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nouns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-w1WEGFrbGM",
        "outputId": "844eb2b2-fbe1-40c2-8582-8deed9838aa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[developer, company]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adjectives"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85GpMARarc57",
        "outputId": "ea284595-9d77-4823-f55d-10d074712780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[interested]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Visualization: Using displaCy"
      ],
      "metadata": {
        "id": "dJJK99IIruy4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy comes with a built-in visualizer called displaCy. We can use it to visualize a dependency parse or named entities in a browser. We can use displaCy to find POS tags for tokens:"
      ],
      "metadata": {
        "id": "-M35s2Iqr1DB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from spacy import displacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "about_interest_text = (\n",
        "    \"He is interested in learning Natural Language Processing.\"\n",
        ")\n",
        "about_interest_doc = nlp(about_interest_text)\n",
        "# displacy.serve(about_interest_doc, style=\"dep\")\n",
        "displacy.render(about_interest_doc, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 333
        },
        "id": "R8ks6h2ureL5",
        "outputId": "dc90f491-caf9-4ead-95da-ec8696d544fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"65d773fcc111437cbdfedb57e379b286-0\" class=\"displacy\" width=\"1450\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">He</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">is</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">AUX</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">interested</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">ADJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">in</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">learning</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">Natural</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">Language</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">Processing.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-65d773fcc111437cbdfedb57e379b286-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-65d773fcc111437cbdfedb57e379b286-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-65d773fcc111437cbdfedb57e379b286-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-65d773fcc111437cbdfedb57e379b286-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M395.0,179.0 L403.0,167.0 387.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-65d773fcc111437cbdfedb57e379b286-0-2\" stroke-width=\"2px\" d=\"M420,177.0 C420,89.5 570.0,89.5 570.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-65d773fcc111437cbdfedb57e379b286-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M570.0,179.0 L578.0,167.0 562.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-65d773fcc111437cbdfedb57e379b286-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-65d773fcc111437cbdfedb57e379b286-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pcomp</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M745.0,179.0 L753.0,167.0 737.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-65d773fcc111437cbdfedb57e379b286-0-4\" stroke-width=\"2px\" d=\"M945,177.0 C945,89.5 1095.0,89.5 1095.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-65d773fcc111437cbdfedb57e379b286-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M945,179.0 L937,167.0 953,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-65d773fcc111437cbdfedb57e379b286-0-5\" stroke-width=\"2px\" d=\"M1120,177.0 C1120,89.5 1270.0,89.5 1270.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-65d773fcc111437cbdfedb57e379b286-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1120,179.0 L1112,167.0 1128,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-65d773fcc111437cbdfedb57e379b286-0-6\" stroke-width=\"2px\" d=\"M770,177.0 C770,2.0 1275.0,2.0 1275.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-65d773fcc111437cbdfedb57e379b286-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1275.0,179.0 L1283.0,167.0 1267.0,167.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Preprocessing Functions"
      ],
      "metadata": {
        "id": "LUqAfz0qNYSC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A preprocessing function converts text to an analyzable format. It's typical for most NLP tasks. Here's an example:"
      ],
      "metadata": {
        "id": "64gHrP-sNfnS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "complete_text = (\n",
        "    \"Gus Proto is a Python developer currently\"\n",
        "    \" working for a London-based Fintech company. He is\"\n",
        "    \" interested in learning Natural Language Processing.\"\n",
        "    \" There is a developer conference happening on 21 July\"\n",
        "    ' 2019 in London. It is titled \"Applications of Natural'\n",
        "    ' Language Processing\". There is a helpline number'\n",
        "    \" available at +44-1234567891. Gus is helping organize it.\"\n",
        "    \" He keeps organizing local Python meetups and several\"\n",
        "    \" internal talks at his workplace. Gus is also presenting\"\n",
        "    ' a talk. The talk will introduce the reader about \"Use'\n",
        "    ' cases of Natural Language Processing in Fintech\".'\n",
        "    \" Apart from his work, he is very passionate about music.\"\n",
        "    \" Gus is learning to play the Piano. He has enrolled\"\n",
        "    \" himself in the weekend batch of Great Piano Academy.\"\n",
        "    \" Great Piano Academy is situated in Mayfair or the City\"\n",
        "    \" of London and has world-class piano instructors.\"\n",
        ")"
      ],
      "metadata": {
        "id": "MOXkMohjNaFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete_doc = nlp(complete_text)\n",
        "def is_token_allowed(token):\n",
        "    return bool(\n",
        "        token\n",
        "        and str(token).strip()\n",
        "        and not token.is_stop\n",
        "        and not token.is_punct\n",
        "    )\n",
        "\n",
        "def preprocess_token(token):\n",
        "    return token.lemma_.strip().lower()\n",
        "\n",
        "complete_filtered_tokens = [\n",
        "    preprocess_token(token)\n",
        "    for token in complete_doc\n",
        "    if is_token_allowed(token)\n",
        "]\n",
        "\n",
        "complete_filtered_tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPkgDnWpNnDh",
        "outputId": "405ca214-acf2-4eb4-d2c4-71526d2a37bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['gus',\n",
              " 'proto',\n",
              " 'python',\n",
              " 'developer',\n",
              " 'currently',\n",
              " 'work',\n",
              " 'london',\n",
              " 'base',\n",
              " 'fintech',\n",
              " 'company',\n",
              " 'interested',\n",
              " 'learn',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'developer',\n",
              " 'conference',\n",
              " 'happen',\n",
              " '21',\n",
              " 'july',\n",
              " '2019',\n",
              " 'london',\n",
              " 'title',\n",
              " 'application',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'helpline',\n",
              " 'number',\n",
              " 'available',\n",
              " '+44',\n",
              " '1234567891',\n",
              " 'gus',\n",
              " 'helping',\n",
              " 'organize',\n",
              " 'keep',\n",
              " 'organize',\n",
              " 'local',\n",
              " 'python',\n",
              " 'meetup',\n",
              " 'internal',\n",
              " 'talk',\n",
              " 'workplace',\n",
              " 'gus',\n",
              " 'present',\n",
              " 'talk',\n",
              " 'talk',\n",
              " 'introduce',\n",
              " 'reader',\n",
              " 'use',\n",
              " 'case',\n",
              " 'natural',\n",
              " 'language',\n",
              " 'processing',\n",
              " 'fintech',\n",
              " 'apart',\n",
              " 'work',\n",
              " 'passionate',\n",
              " 'music',\n",
              " 'gus',\n",
              " 'learn',\n",
              " 'play',\n",
              " 'piano',\n",
              " 'enrol',\n",
              " 'weekend',\n",
              " 'batch',\n",
              " 'great',\n",
              " 'piano',\n",
              " 'academy',\n",
              " 'great',\n",
              " 'piano',\n",
              " 'academy',\n",
              " 'situate',\n",
              " 'mayfair',\n",
              " 'city',\n",
              " 'london',\n",
              " 'world',\n",
              " 'class',\n",
              " 'piano',\n",
              " 'instructor']"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Rule-Based Matching Using spaCy"
      ],
      "metadata": {
        "id": "8yaMWL-7Nt0M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rule-based matching is one of the steps in extracting information from unstructured text. It's used to identify and extract tokens and phrases according to patterns (such as lowercase) and grammatical features (such as part of speech).\n",
        "\n",
        "While you can use regular expressions to extract entities (such as phone numbers), rule-based matching in spaCy is more powerful than regex alone, because you can include semantic or grammatical filters.\n",
        "\n",
        "For example, with rule-based matching, you can extract a first name and a last name, which are always proper nouns:"
      ],
      "metadata": {
        "id": "xlDZcaCoN3bW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "about_text = (\n",
        "    \"Gus Proto is a Python developer currently\"\n",
        "    \" working for a London-based Fintech\"\n",
        "    \" company. He is interested in learning\"\n",
        "    \" Natural Language Processing.\"\n",
        ")\n",
        "about_doc = nlp(about_text)\n"
      ],
      "metadata": {
        "id": "EiUw45OYNpus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.matcher import Matcher\n",
        "matcher = Matcher(nlp.vocab)\n",
        "\n",
        "def extract_full_name(nlp_doc):\n",
        "    pattern = [{\"POS\": \"PROPN\"}, {\"POS\": \"PROPN\"}]\n",
        "    matcher.add(\"FULL_NAME\", [pattern])\n",
        "    matches = matcher(nlp_doc)\n",
        "    for _, start, end in matches:\n",
        "        span = nlp_doc[start:end]\n",
        "        yield span.text\n",
        "\n"
      ],
      "metadata": {
        "id": "RfAnhmrwN8q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(extract_full_name(about_doc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "G_1EZiq1N-L0",
        "outputId": "a53f5ff2-4e2e-44e5-bd9c-a7ee779ae03c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Gus Proto'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Dependency Parsing Using spaCy"
      ],
      "metadata": {
        "id": "RjupJz9TOteM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependency parsing is the process of extracting the dependency graph of a sentence to represent its grammatical structure. It defines the dependency relationship between headwords and their dependents. The head of a sentence has no dependency and is called the root of the sentence. The verb is usually the root of the sentence. All other words are linked to the headword.\n",
        "\n",
        "The dependencies can be mapped in a directed graph representation where:\n",
        "\n",
        "Words are the nodes.\n",
        "Grammatical relationships are the edges.\n",
        "Dependency parsing helps you know what role a word plays in the text and how different words relate to each other.\n",
        "\n",
        "Here's how you can use dependency parsing to find the relationships between words:"
      ],
      "metadata": {
        "id": "xDck_J8TO_KK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "piano_text = \"Gus is learning piano\"\n",
        "piano_doc = nlp(piano_text)\n",
        "for token in piano_doc:\n",
        "    print(\n",
        "        f\"\"\"\n",
        "TOKEN: {token.text}\n",
        "=====\n",
        "{token.tag_ = }\n",
        "{token.head.text = }\n",
        "{token.dep_ = }\"\"\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92l1ju0gOTpS",
        "outputId": "cf5e37c4-7b5e-4f9e-fb1e-c960c5313daf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "TOKEN: Gus\n",
            "=====\n",
            "token.tag_ = 'NNP'\n",
            "token.head.text = 'learning'\n",
            "token.dep_ = 'nsubj'\n",
            "\n",
            "TOKEN: is\n",
            "=====\n",
            "token.tag_ = 'VBZ'\n",
            "token.head.text = 'learning'\n",
            "token.dep_ = 'aux'\n",
            "\n",
            "TOKEN: learning\n",
            "=====\n",
            "token.tag_ = 'VBG'\n",
            "token.head.text = 'learning'\n",
            "token.dep_ = 'ROOT'\n",
            "\n",
            "TOKEN: piano\n",
            "=====\n",
            "token.tag_ = 'NN'\n",
            "token.head.text = 'learning'\n",
            "token.dep_ = 'dobj'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the sentence contains three relationships:\n",
        "\n",
        "nsubj is the subject of the word, and its headword is a verb.\n",
        "aux is an auxiliary word, and its headword is a verb.\n",
        "dobj is the direct object of the verb, and its headword is also a verb.\n",
        "The list of relationships isn't particular to spaCy. Rather, it's an evolving field of linguistics research.\n",
        "\n",
        "You can also use displaCy to visualize the dependency tree of the sentence:"
      ],
      "metadata": {
        "id": "O1zuprn4woIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.serve(piano_doc, style=\"dep\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W4B5fZbwws_D",
        "outputId": "22f6d8d5-ed09-4d3a-c518-48a120dd49c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shutting down server on port 5000.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tree and Subtree Navigation"
      ],
      "metadata": {
        "id": "4Dejx_lZkAW7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dependency graph has all the properties of a tree. This tree contains information about sentence structure and grammar and can be traversed in different ways to extract relationships.\n",
        "\n",
        "spaCy provides attributes like .children, .lefts, .rights, and .subtree to make navigating the parse tree easier. Here are a few examples of using those attributes:"
      ],
      "metadata": {
        "id": "5I2tJO6IkFYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "one_line_about_text = (\n",
        "    \"Gus Proto is a Python developer\"\n",
        "    \" currently working for a London-based Fintech company\"\n",
        ")\n",
        "one_line_about_doc = nlp(one_line_about_text)\n",
        "\n",
        "# Extract children of `developer`\n",
        "print([token.text for token in one_line_about_doc[5].children])\n",
        "\n",
        "\n",
        "# Extract previous neighboring node of `developer`\n",
        "print (one_line_about_doc[5].nbor(-1))\n",
        "\n",
        "\n",
        "# Extract next neighboring node of `developer`\n",
        "print (one_line_about_doc[5].nbor())\n",
        "\n",
        "\n",
        "# Extract all tokens on the left of `developer`\n",
        "print([token.text for token in one_line_about_doc[5].lefts])\n",
        "\n",
        "\n",
        "# Extract tokens on the right of `developer`\n",
        "print([token.text for token in one_line_about_doc[5].rights])\n",
        "\n",
        "\n",
        "# Print subtree of `developer`\n",
        "print (list(one_line_about_doc[5].subtree))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6kjhC_ukCtH",
        "outputId": "e29efca7-417a-48e3-f63e-7781ccfae709"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'Python', 'working']\n",
            "Python\n",
            "currently\n",
            "['a', 'Python']\n",
            "['working']\n",
            "[a, Python, developer, currently, working, for, a, London, -, based, Fintech, company]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Shallow Parsing"
      ],
      "metadata": {
        "id": "k0UdAstQ7Lrw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Shallow parsing, or chunking, is the process of extracting phrases from unstructured text. This involves chunking groups of adjacent tokens into phrases on the basis of their POS tags. There are some standard well-known chunks such as noun phrases, verb phrases, and prepositional phrases."
      ],
      "metadata": {
        "id": "xXwOVah27LK2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Noun Phrase Detection\n",
        "A noun phrase is a phrase that has a noun as its head. It could also include other kinds of words, such as adjectives, ordinals, and determiners. Noun phrases are useful for explaining the context of the sentence. They help you understand what the sentence is about.\n",
        "\n",
        "spaCy has the property .noun_chunks on the Doc object. You can use this property to extract noun phrases:"
      ],
      "metadata": {
        "id": "9aHnnAo57SCD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "conference_text = (\n",
        "    \"There is a developer conference happening on 21 July 2019 in London.\"\n",
        ")\n",
        "conference_doc = nlp(conference_text)\n",
        "\n",
        "# Extract Noun Phrases\n",
        "for chunk in conference_doc.noun_chunks:\n",
        "    print (chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7yPyZyb7S1u",
        "outputId": "9f566ad9-febb-428d-e762-ee25ebe68a10"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a developer conference\n",
            "21 July\n",
            "London\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verb Phrase Detection**"
      ],
      "metadata": {
        "id": "sfLpYxlM7dPs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A verb phrase is a syntactic unit composed of at least one verb. This verb can be joined by other chunks, such as noun phrases. Verb phrases are useful for understanding the actions that nouns are involved in.\n",
        "\n",
        "spaCy has no built-in functionality to extract verb phrases, so you’ll need a library called textacy. You can use pip to install textacy:"
      ],
      "metadata": {
        "id": "VtnNkW4_7gRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rxd-7K4y7ilb",
        "outputId": "763d0e47-7711-4bfb-8e72-d365cfd9ca9d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textacy\n",
            "  Downloading textacy-0.13.0-py3-none-any.whl (210 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cachetools>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (5.3.1)\n",
            "Requirement already satisfied: catalogue~=2.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (2.0.9)\n",
            "Collecting cytoolz>=0.10.1 (from textacy)\n",
            "  Downloading cytoolz-0.12.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting floret~=0.10.0 (from textacy)\n",
            "  Downloading floret-0.10.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (320 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.3/320.3 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jellyfish>=0.8.0 (from textacy)\n",
            "  Downloading jellyfish-1.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.3.2)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.10/dist-packages (from textacy) (3.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.23.5)\n",
            "Collecting pyphen>=0.10.0 (from textacy)\n",
            "  Downloading pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.10.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (1.2.2)\n",
            "Requirement already satisfied: spacy~=3.0 in /usr/local/lib/python3.10/dist-packages (from textacy) (3.6.1)\n",
            "Requirement already satisfied: tqdm>=4.19.6 in /usr/local/lib/python3.10/dist-packages (from textacy) (4.66.1)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from cytoolz>=0.10.1->textacy) (0.12.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.10.0->textacy) (2023.7.22)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0->textacy) (3.2.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (2.4.7)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (6.3.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (2.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy~=3.0->textacy) (3.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (2.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy~=3.0->textacy) (4.7.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy~=3.0->textacy) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy~=3.0->textacy) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy~=3.0->textacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy~=3.0->textacy) (2.1.3)\n",
            "Installing collected packages: pyphen, jellyfish, floret, cytoolz, textacy\n",
            "Successfully installed cytoolz-0.12.2 floret-0.10.4 jellyfish-1.0.0 pyphen-0.14.0 textacy-0.13.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import textacy\n",
        "\n",
        "about_talk_text = (\n",
        "    \"The talk will introduce reader about use\"\n",
        "    \" cases of Natural Language Processing in\"\n",
        "    \" Fintech, making use of\"\n",
        "    \" interesting examples along the way.\"\n",
        ")\n",
        "\n",
        "patterns = [{\"POS\": \"AUX\"}, {\"POS\": \"VERB\"}]\n",
        "about_talk_doc = textacy.make_spacy_doc(\n",
        "    about_talk_text, lang=\"en_core_web_sm\"\n",
        ")\n",
        "verb_phrases = textacy.extract.token_matches(\n",
        "    about_talk_doc, patterns=patterns\n",
        ")\n",
        "\n",
        "# Print all verb phrases\n",
        "for chunk in verb_phrases:\n",
        "    print(chunk.text)\n",
        "\n",
        "\n",
        "\n",
        "# Extract noun phrase to explain what nouns are involved\n",
        "for chunk in about_talk_doc.noun_chunks:\n",
        "    print (chunk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDQvGj4v7lqo",
        "outputId": "e3c9fc99-a522-4aa6-92f2-a3f36b0079f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "will introduce\n",
            "The talk\n",
            "reader\n",
            "use cases\n",
            "Natural Language Processing\n",
            "Fintech\n",
            "use\n",
            "interesting examples\n",
            "the way\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, the verb phrase introduce indicates that something will be introduced. By looking at the noun phrases, you can piece together what will be introduced—again, without having to read the whole text."
      ],
      "metadata": {
        "id": "bhCpn5do7qR0"
      }
    }
  ]
}