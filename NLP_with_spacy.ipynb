{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM8Qf3pbu5YnZgfRz6vrc9s",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ajits-github/NLP_with_spacy/blob/main/NLP_with_spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJ5vpvOUiAue",
        "outputId": "77186afa-d04c-46ac-f30c-c09ab222543c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<spacy.lang.en.English at 0x78b0c1c0c8b0>"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "nlp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "sImX6lzfj50h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###The Doc Object for Processed Text"
      ],
      "metadata": {
        "id": "Yf8yISWbjdk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "introduction_doc = nlp(\n",
        "...     \"This tutorial is about Natural Language Processing in spaCy.\"\n",
        "... )\n",
        "type(introduction_doc)\n",
        "[token.text for token in introduction_doc]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UF6F4WSliNnz",
        "outputId": "40fc18c6-1993-4389-a2d5-5c3cce9ea648"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['This',\n",
              " 'tutorial',\n",
              " 'is',\n",
              " 'about',\n",
              " 'Natural',\n",
              " 'Language',\n",
              " 'Processing',\n",
              " 'in',\n",
              " 'spaCy',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pathlib\n",
        "file_name = \"/content/introduction.txt\"\n",
        "introduction_doc = nlp(pathlib.Path(file_name).read_text(encoding=\"utf-8\"))\n",
        "print ([token.text for token in introduction_doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKYoqD99is0H",
        "outputId": "8ebd2513-145c-4bbc-d6ea-4e232e019152"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\"', 'This', 'tutorial', 'is', 'about', 'Natural', 'Language', 'Processing', 'in', 'spaCy', '.', '\"']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "ylocSCVNj4pH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sentence Detection"
      ],
      "metadata": {
        "id": "RidNRn9EjX-X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "about_text = (\n",
        "    \"Gus Proto is a Python developer currently\"\n",
        "    \" working for a London-based Fintech\"\n",
        "    \" company. He is interested in learning\"\n",
        "    \" Natural Language Processing.\"\n",
        ")\n",
        "about_doc = nlp(about_text)\n",
        "sentences = list(about_doc.sents)\n",
        "len(sentences)\n",
        "\n",
        "for sentence in sentences:\n",
        "    print(f\"{sentence[:5]}...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KfFWWWJMjLpI",
        "outputId": "f04a31dd-2bbc-49e0-919e-adc2754e8ebd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gus Proto is a Python...\n",
            "He is interested in learning...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ellipsis_text = (\n",
        "    \"Gus, can you, ... never mind, I forgot\"\n",
        "    \" what I was saying. So, do you think\"\n",
        "    \" we should ...\"\n",
        ")\n",
        "\n",
        "from spacy.language import Language\n",
        "@Language.component(\"set_custom_boundaries\")\n",
        "def set_custom_boundaries(doc):\n",
        "    \"\"\"Add support to use `...` as a delimiter for sentence detection\"\"\"\n",
        "    for token in doc[:-1]:\n",
        "        if token.text == \"...\":\n",
        "            doc[token.i + 1].is_sent_start = True\n",
        "    return doc\n",
        "\n",
        "\n",
        "custom_nlp = spacy.load(\"en_core_web_sm\")\n",
        "custom_nlp.add_pipe(\"set_custom_boundaries\", before=\"parser\")\n",
        "custom_ellipsis_doc = custom_nlp(ellipsis_text)\n",
        "custom_ellipsis_sentences = list(custom_ellipsis_doc.sents)\n",
        "for sentence in custom_ellipsis_sentences:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4V3fefeejTKQ",
        "outputId": "d5b16c46-936e-47fd-b571-323c52e58e93"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gus, can you, ...\n",
            "never mind, I forgot what I was saying.\n",
            "So, do you think we should ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* We used the @Language.component(\"set_custom_boundaries\") decorator to define a new function that takes a Doc object as an argument. The job of this function is to identify tokens in Doc that are the beginning of sentences and mark their .is_sent_start attribute to True. Once done, the function must return the Doc object again"
      ],
      "metadata": {
        "id": "sCflvNsCjyCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "9cu-pL5cj7ym"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Tokens in spaCy"
      ],
      "metadata": {
        "id": "swqnwUIlkSt3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building the Doc container involves tokenizing the text. The process of tokenization breaks a text down into its basic units—or tokens—which are represented in spaCy as Token objects."
      ],
      "metadata": {
        "id": "fdM8jEVZkXjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "about_text = (\n",
        "    \"Gus Proto is a Python developer currently\"\n",
        "    \" working for a London-based Fintech\"\n",
        "    \" company. He is interested in learning\"\n",
        "    \" Natural Language Processing.\"\n",
        ")\n",
        "about_doc = nlp(about_text)\n",
        "\n",
        "for token in about_doc:\n",
        "    print (token, token.idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEgHFegaj9Ft",
        "outputId": "0e8ed91b-9cb3-493a-b194-22014ab5a50f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gus 0\n",
            "Proto 4\n",
            "is 10\n",
            "a 13\n",
            "Python 15\n",
            "developer 22\n",
            "currently 32\n",
            "working 42\n",
            "for 50\n",
            "a 54\n",
            "London 56\n",
            "- 62\n",
            "based 63\n",
            "Fintech 69\n",
            "company 77\n",
            ". 84\n",
            "He 86\n",
            "is 89\n",
            "interested 92\n",
            "in 103\n",
            "learning 106\n",
            "Natural 115\n",
            "Language 123\n",
            "Processing 132\n",
            ". 142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{\"Text with Whitespace\":22}'\n",
        "      f'{\"Is Alphanumeric?\":15}'\n",
        "      f'{\"Is Punctuation?\":18}'\n",
        "      f'{\"Is Stop Word?\"}'\n",
        ")\n",
        "\n",
        "for token in about_doc:\n",
        "    print(\n",
        "        f\"{str(token.text_with_ws):22}\"\n",
        "        f\"{str(token.is_alpha):15}\"\n",
        "        f\"{str(token.is_punct):18}\"\n",
        "        f\"{str(token.is_stop)}\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0lrjzWLEkea8",
        "outputId": "a7d062f2-7cd1-4093-ffc6-f094bbe7f68a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Text with Whitespace  Is Alphanumeric?Is Punctuation?   Is Stop Word?\n",
            "Gus                   True           False             False\n",
            "Proto                 True           False             False\n",
            "is                    True           False             True\n",
            "a                     True           False             True\n",
            "Python                True           False             False\n",
            "developer             True           False             False\n",
            "currently             True           False             False\n",
            "working               True           False             False\n",
            "for                   True           False             True\n",
            "a                     True           False             True\n",
            "London                True           False             False\n",
            "-                     False          True              False\n",
            "based                 True           False             False\n",
            "Fintech               True           False             False\n",
            "company               True           False             False\n",
            ".                     False          True              False\n",
            "He                    True           False             True\n",
            "is                    True           False             True\n",
            "interested            True           False             False\n",
            "in                    True           False             True\n",
            "learning              True           False             False\n",
            "Natural               True           False             False\n",
            "Language              True           False             False\n",
            "Processing            True           False             False\n",
            ".                     False          True              False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* text_with_ws prints the token text along with any trailing space, if present\n",
        "* is_alpha indicates whether the token consists of alphabetic characters or not\n",
        "* is_punct indicates whether the token is a punctuation symbol or not\n",
        "* is_stop indicates whether the token is a stop word or not"
      ],
      "metadata": {
        "id": "s4KkWd-VklMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "custom_about_text = (\n",
        "    \"Gus Proto is a Python developer currently\"\n",
        "    \" working for a London@based Fintech\"\n",
        "    \" company. He is interested in learning\"\n",
        "    \" Natural Language Processing.\"\n",
        ")\n",
        "\n",
        "print([token.text for token in nlp(custom_about_text)[8:15]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRcNtGslkvCX",
        "outputId": "c97f5e60-0aa2-4fa1-e4aa-6fd1da7a5a23"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['for', 'a', 'London@based', 'Fintech', 'company', '.', 'He']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* As with many aspects of spaCy, we can also customize the tokenization process to detect tokens on custom characters. This is often used for hyphenated words such as London-based.\n",
        "\n",
        "* To customize tokenization, you need to update the tokenizer property on the callable Language object with a new Tokenizer object.\n",
        "\n",
        "* To see what's involved, imagine you had some text that used the @ symbol instead of the usual hyphen (-) as an infix to link words together. So, instead of London-based, we had London@based\n",
        "\n",
        "* In this example, the default parsing read the London@based text as a single token, but if we had used a hyphen instead of the @ symbol, then we'd get three tokens.\n",
        "\n",
        "* To include the @ symbol as a custom infix, we need to build your own Tokenizer object"
      ],
      "metadata": {
        "id": "z9G5wuZYmbtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from spacy.tokenizer import Tokenizer\n",
        "\n",
        "custom_nlp = spacy.load(\"en_core_web_sm\")\n",
        "prefix_re = spacy.util.compile_prefix_regex(\n",
        "    custom_nlp.Defaults.prefixes\n",
        ")\n",
        "suffix_re = spacy.util.compile_suffix_regex(\n",
        "    custom_nlp.Defaults.suffixes\n",
        ")\n",
        "\n",
        "custom_infixes = [r\"@\"]\n",
        "\n",
        "infix_re = spacy.util.compile_infix_regex(\n",
        "    list(custom_nlp.Defaults.infixes) + custom_infixes\n",
        ")\n",
        "\n",
        "custom_nlp.tokenizer = Tokenizer(\n",
        "    nlp.vocab,\n",
        "    prefix_search=prefix_re.search,\n",
        "    suffix_search=suffix_re.search,\n",
        "    infix_finditer=infix_re.finditer,\n",
        "    token_match=None,\n",
        ")\n",
        "\n",
        "custom_tokenizer_about_doc = custom_nlp(custom_about_text)\n",
        "\n",
        "print([token.text for token in custom_tokenizer_about_doc[8:15]])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICjqNvs2mh-h",
        "outputId": "eb6fa3fc-6c16-4627-e63a-6c649d4c2055"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['for', 'a', 'London', '@', 'based', 'Fintech', 'company']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Stop Words"
      ],
      "metadata": {
        "id": "EueV0XGenAvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stop words are typically defined as the most common words in a language. In the English language, some examples of stop words are the, are, but, and they. Most sentences need to contain stop words in order to be full sentences that make grammatical sense.\n",
        "\n",
        "With NLP, stop words are generally removed because they aren’t significant, and they heavily distort any word frequency analysis. spaCy stores a list of stop words for the English language"
      ],
      "metadata": {
        "id": "lch4S9vOnIeh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "len(spacy_stopwords)\n",
        "\n",
        "for stop_word in list(spacy_stopwords)[:10]:\n",
        "    print(stop_word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JUC5FgSOm850",
        "outputId": "df0e52da-c0bb-4cbc-ba5a-1a0a3086f375"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "across\n",
            "'ve\n",
            "hers\n",
            "beforehand\n",
            "re\n",
            "six\n",
            "namely\n",
            "is\n",
            "’ve\n",
            "we\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "custom_about_text = (\n",
        "    \"Gus Proto is a Python developer currently\"\n",
        "    \" working for a London-based Fintech\"\n",
        "    \" company. He is interested in learning\"\n",
        "    \" Natural Language Processing.\"\n",
        ")\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "about_doc = nlp(custom_about_text)\n",
        "print([token for token in about_doc if not token.is_stop])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7IMlgwBnMy-",
        "outputId": "0759e525-4a57-47cf-b2c4-6e6c26dd610e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Gus, Proto, Python, developer, currently, working, London, -, based, Fintech, company, ., interested, learning, Natural, Language, Processing, .]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Lemmatization"
      ],
      "metadata": {
        "id": "bEN54-6InQL6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization is the process of reducing inflected forms of a word while still ensuring that the reduced form belongs to the language. This reduced form, or root word, is called a lemma."
      ],
      "metadata": {
        "id": "vTK_GognnS_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "conference_help_text = (\n",
        "    \"Gus is helping organize a developer\"\n",
        "    \" conference on Applications of Natural Language\"\n",
        "    \" Processing. He keeps organizing local Python meetups\"\n",
        "    \" and several internal talks at his workplace.\"\n",
        ")\n",
        "conference_help_doc = nlp(conference_help_text)\n",
        "for token in conference_help_doc:\n",
        "    if str(token) != str(token.lemma_):\n",
        "        print(f\"{str(token):>20} : {str(token.lemma_)}\")"
      ],
      "metadata": {
        "id": "2B7xwB85nRYL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}